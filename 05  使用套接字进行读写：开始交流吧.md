# 05 | 使用套接字进行读写：开始交流吧

**连接建立的根本目的是为了数据的收发**。拿我们常用的网购场景举例子，我们在浏览商品或者购买货品的时候，并不会察觉到网络连接的存在，但是我们可以真切感觉到数据在客户端和服务器端有效的传送， 比如浏览商品时商品信息的不断刷新，购买货品时显示购买成功的消息等。

### 

### 发送数据

​		发送数据时常用的有三个函数，分别是 write、send 和 sendmsg。

~~~c

ssize_t write (int socketfd, const void *buffer, size_t size)
ssize_t send (int socketfd, const void *buffer, size_t size, int flags)
ssize_t sendmsg(int sockfd, const struct msghdr *msg, int flags)
~~~

​		每个函数都是单独使用的，使用的场景略有不同：

​		第一个函数是常见的**文件写函数**，如果把 socketfd 换成文件描述符，就是普通的文件写入。

​		如果想指定选项，**发送带外数据，就需要使用第二个带 flag 的函数**。所谓带外数据，是一种基于 TCP 协议的紧急数据，用于客户端 - 服务器在特定场景下的紧急处理。

​		如果想指定**多重缓冲区传输数据，就需要使用第三个函数**，以结构体 msghdr 的方式发送数据。

​		你看到这里可能会问，既然套接字描述符是一种特殊的描述符，那么在套接字描述符上调用 write 函数，应该和在普通文件描述符上调用 write 函数的行为是一致的，都是通过描述符句柄写入指定的数据

​		乍一看，两者的表现形式是一样，内在的区别还是很不一样的。

​		**对于普通文件描述符而言，一个文件描述符代表了打开的一个文件句柄，通过调用 write 函数，操作系统内核帮我们不断地往文件系统中写入字节流。注意，写入的字节流大小通常和输入参数 size 的值是相同的，否则表示出错。**



​		对于套接字描述符而言，它代表了一个**双向连接**，在套接字描述符上调用 write 写入的字节数**有可能**比请求的数量少，这在普通文件描述符情况下是不正常的。

​	这里有几种情况：

​		第一种情况很简单，操作系统内核的发送缓冲区足够大，可以直接容纳这份数据，那么皆大欢喜，我们的程序从 write 调用中退出，返回写入的字节数就是应用程序的数据大小。

​		第二种情况是，操作系统内核的发送缓冲区是够大了，不过**还有数据没有发送完**，或者数据发送完了，但是操作系统内核的**发送缓冲区不足以容纳应用程序数据**，在这种情况下，你预料的结果是什么呢？报错？还是直接返回？

​		操作系统**内核并不会返回，也不会报错，而是应用程序被阻塞**，也就是说应用程序在 write 函数调用处停留，不直接返回。术语“挂起”也表达了相同的意思，不过“挂起”是从操作系统内核角度来说的。



那么什么时候才会返回呢？

​		实际上，每个操作系统内核的处理是不同的。大部分 **UNIX 系统的做法是一直等到可以把应用程序数据完全放到操作系统内核的发送缓冲区中，再从系统调用中返回**。怎么理解呢？



​		别忘了，我们的操作系统内核是很聪明的，**当 TCP 连接建立之后**，它就开始运作起来。你可以把发送缓冲区想象成一条包裹流水线，有个聪明且忙碌的工人不断地从流水线上取出包裹（数据），这个工人会**按照 TCP/IP 的语义，将取出的包裹（数据）封装成 TCP 的 MSS 包，以及 IP 的 MTU 包**，最后**走数据链路层将数据发送出去**。这样我们的**发送缓冲区就又空了一部分，于是又可以继续从应用程序搬一部分数据到发送缓冲区里，这样一直进行下去，到某一个时刻，应用程序的数据可以完全放置到发送缓冲区里**。在这个时候，**write 阻塞调用返回**。**注意返回的时刻，应用程序数据并没有全部被发送出去，发送缓冲区里还有部分数据，这部分数据会在稍后由操作系统内核通过网络发送出去。**

![](image\发送数据示意图.png)

### 读取数据

​		我们可以注意到，**套接字描述本身和本地文件描述符并无区别，在 UNIX 的世界里万物都是文件**，这就意味着可以将套接字描述符传递给那些原先为处理本地文件而设计的函数。这些函数包括 **read 和 write 交换数据的函数。**

让我们先从最简单的 read 函数开始看起，这个函数的原型如下：

~~~c
/* 从socketfd描述字中读取"size"个字节. */
size_t readn(int fd, void *buffer, size_t size) {
    char *buffer_pointer = buffer;
    int length = size;

    while (length > 0) {
        int result = read(fd, buffer_pointer, length);

        if (result < 0) {
            if (errno == EINTR)
                continue;     /* 考虑非阻塞的情况，这里需要再次调用read */
            else
                return (-1);
        } else if (result == 0)
            break;                /* EOF(End of File)表示套接字关闭 */

        length -= result;
        buffer_pointer += result;
    }
    return (size - length);        /* 返回的是实际读取的字节数*/
}
~~~

对这个程序稍微解释下：

- 6-19 行的循环条件表示的是，在没读满 size 个字节之前，一直都要循环下去。
- 10-11 行表示的是**非阻塞 I/O 的情况**下，没有数据可以读，需要继续调用 read。
- 14-15 行表示读到对方发出的 FIN 包，表现形式是 EOF，此时需要关闭套接字
- 17-18 行，需要读取的字符数减少，缓存指针往下移动。
- 20 行是在读取 EOF 跳出循环后，返回实际读取的字符数。

### 缓冲区实验

​		我们用一个**客户端 - 服务器**的例子来解释一下读取缓冲区和发送缓冲区的概念。在这个例子中客户端不断地发送数据，服务器端每读取一段数据之后进行休眠，以模拟实际业务处理所需要的时间。

#### 服务器端读取数据程序

下面是服务器端读取数据的程序：

~~~c
#include "lib/common.h"

void read_data(int sockfd) {
    ssize_t n;
    char buf[1024];

    int time = 0;
    for (;;) {
        fprintf(stdout, "block in read\n");
        if ((n = readn(sockfd, buf, 1024)) == 0)
            return;

        time++;
        fprintf(stdout, "1K read for %d \n", time);
        usleep(1000);
    }
}


int main(int argc, char **argv) {
    int listenfd, connfd;
    socklen_t clilen;
    struct sockaddr_in cliaddr, servaddr;

    listenfd = socket(AF_INET, SOCK_STREAM, 0);

    bzero(&servaddr, sizeof(servaddr));
    servaddr.sin_family = AF_INET;
    servaddr.sin_addr.s_addr = htonl(INADDR_ANY);
    servaddr.sin_port = htons(12345);

    /* bind到本地地址，端口为12345 */
    bind(listenfd, (struct sockaddr *) &servaddr, sizeof(servaddr));
    /* listen的backlog为1024 */
    listen(listenfd, 1024);

    /* 循环处理用户请求 */
    for (;;) {
        clilen = sizeof(cliaddr);
        connfd = accept(listenfd, (struct sockaddr *) &cliaddr, &clilen);
        read_data(connfd);   /* 读取数据 */
        close(connfd);          /* 关闭连接套接字，注意不是监听套接字*/
    }
}
~~~

对服务器端程序解释如下：

- 21-35 行先后创建了 socket 套接字，bind 到对应地址和端口，并开始调用 listen 接口监听；
- 38-42 行循环等待连接，通过 accept 获取实际的连接，并开始读取数据；
- 8-15 行实际每次读取 1K 数据，之后休眠 1 秒，用来模拟服务器端处理时延。

### 客户端发送数据程序

下面是客户端发送数据的程序：

~~~c
#include "lib/common.h"

#define MESSAGE_SIZE 102400

void send_data(int sockfd) {
    char *query;
    query = malloc(MESSAGE_SIZE + 1);
    for (int i = 0; i < MESSAGE_SIZE; i++) {
        query[i] = 'a';
    }
    query[MESSAGE_SIZE] = '\0';

    const char *cp;
    cp = query;
    size_t remaining = strlen(query);
    while (remaining) {
        int n_written = send(sockfd, cp, remaining, 0);
        fprintf(stdout, "send into buffer %ld \n", n_written);
        if (n_written <= 0) {
            error(1, errno, "send failed");
            return;
        }
        remaining -= n_written;
        cp += n_written;
    }

    return;
}

int main(int argc, char **argv) {
    int sockfd;
    struct sockaddr_in servaddr;

    if (argc != 2)
        error(1, 0, "usage: tcpclient <IPaddress>");

    sockfd = socket(AF_INET, SOCK_STREAM, 0);

    bzero(&servaddr, sizeof(servaddr));
    servaddr.sin_family = AF_INET;
    servaddr.sin_port = htons(12345);
    inet_pton(AF_INET, argv[1], &servaddr.sin_addr);
    int connect_rt = connect(sockfd, (struct sockaddr *) &servaddr, sizeof(servaddr));
    if (connect_rt < 0) {
        error(1, errno, "connect failed ");
    }
    send_data(sockfd);
    exit(0);
}
~~~

对客户端程序解释如下：

- 31-37 行先后创建了 socket 套接字，调用 connect 向对应服务器端发起连接请求
- 43 行在连接建立成功后，调用 send_data 发送数据
- 6-11 行初始化了一个长度为 MESSAGE_SIZE 的字符串流
- 16-25 行调用 send 函数将 MESSAGE_SIZE 长度的字符串流发送出去

### 实验一: 观察客户端数据发送行为

客户端程序发送了一个很大的字节流，程序运行起来之后，我们会看到服务端不断地在屏幕上打印出读取字节流的过程：

![](image\客户端发送数据场景.jpg)

而客户端**直到最后所有的字节流发送完毕才打印出下面的一句话**，**说明在此之前 send 函数一直都是阻塞的**，也就是说**阻塞式套接字最终发送返回的实际写入字节数和请求字节数是相等的**。

### 实验二: 服务端处理变慢



如果我们把服务端的休眠时间稍微调大，把客户端发送的字节数从从 10240000 调整为 1024000，再次运行刚才的例子，我们会发现，客户端很快打印出一句话：

![](image\服务端处理变慢场景.jpg)

但与此同时，服务端读取程序还在屏幕上不断打印读取数据的进度，显示出服务端读取程序还在辛苦地从缓冲区中读取数据。通过这个例子我想再次强调一下：

**发送成功仅仅表示的是数据被拷贝到了发送缓冲区中，并不意味着连接对端已经收到所有的数据。至于什么时候发送到对端的接收缓冲区，或者更进一步说，什么时候被对方应用程序缓冲所接收，对我们而言完全都是透明的。**





### 总结

- 对于 send 来说，返回成功仅仅表示数据写到发送缓冲区成功，并不表示对端已经成功收到。
- 对于 read 来说，需要循环读取数据，并且需要考虑 EOF 等异常条件。

### 思考题

既然缓冲区如此重要，我们可不可以把缓冲区搞得大大的，这样不就可以提高应用程序的吞吐量了么？你可以想一想这个方法可行吗？另外你可以自己总结一下，一段数据流从应用程序发送端，一直到应用程序接收端，总共经过了多少次拷贝？

~~~markdown
无限大肯定是不行的，这要从为什么使用缓存这个角度考虑。内核协议栈不确定用户一次要发多少数据，如果用户来一次就发一次，如果数据多还好说，如果少了，那网络I/O很频繁，而真正发送出去的数据也不多，所以为了减少网络I/O使用了缓存的策略。但为啥不呢无限大呢，网卡一次发出去的数据报它是有一个最大长度的，所以你不管累积再多数据最后还是要分片发送的，这样一来缓冲区太大也没什么意义，而且数据传输也是有延时要求的，不可能总是在缓冲区里待着等数据，这样就总会有空出来的缓冲区存放新数据，所以无限大缓冲区也没意义，反而还浪费资源。

发送端，假设数据能一次性复制完，那么从用户态内存拷贝到内核态内存是一次（这里应该直接拷贝到发送换冲区了），传输层组TCP包是第二次拷贝，因为要加包头，而发送缓冲区的都是紧凑内存全是应用层数据，那么分装包就需要一次拷贝，第三次，一个TCP包封装为IP报文这里可能也会需要一次拷贝，毕竟这里走到协议栈的下一层了。
~~~

